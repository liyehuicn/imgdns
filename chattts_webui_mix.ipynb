{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liyehuicn/imgdns/blob/master/chattts_webui_mix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸŒŸ å¦‚æœä½ è§‰å¾— ChatTTS å’Œ ChatTTS_colab é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·è®¿é—®ä»¥ä¸‹é“¾æ¥ç»™å®ƒä»¬ç‚¹ä¸ªæ˜Ÿæ˜Ÿå§ï¼ğŸŒŸ\n",
        "\n",
        "- [ChatTTS é¡¹ç›®](https://github.com/2noise/ChatTTS)\n",
        "\n",
        "- [ChatTTS_colab é¡¹ç›®](https://github.com/6drf21e/ChatTTS_colab)\n",
        "\n",
        "æ„Ÿè°¢ä½ çš„æ”¯æŒï¼\n",
        "\n",
        "# è¿è¡Œæ–¹æ³•\n",
        "\n",
        "- ç‚¹å‡»èœå•æ çš„--ä»£ç æ‰§è¡Œç¨‹åº--å…¨éƒ¨è¿è¡Œå³å¯\n",
        "- æ‰§è¡Œååœ¨ä¸‹æ–¹çš„æ—¥å¿—ä¸­æ‰¾åˆ°ç±»ä¼¼\n",
        "\n",
        "  Running on public URL: https://**************.gradio.live  <-è¿™ä¸ªå°±æ˜¯å¯ä»¥è®¿é—®çš„å…¬ç½‘åœ°å€\n",
        "\n",
        "å®‰è£…åŒ…çš„æ—¶å€™æç¤ºè¦é‡å¯ è¯·ç‚¹**\"å¦\"**"
      ],
      "metadata": {
        "id": "Xo3k5XsTzWK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/6drf21e/ChatTTS_colab\n",
        "%cd ChatTTS_colab\n",
        "!git clone -q https://github.com/2noise/ChatTTS\n",
        "%cd ChatTTS\n",
        "!git checkout -q e6412b1\n",
        "%cd ..\n",
        "!mv ChatTTS abc\n",
        "!mv abc/* /content/ChatTTS_colab/\n",
        "!pip install -q omegaconf vocos vector_quantize_pytorch gradio cn2an pypinyin openai jieba WeTextProcessing python-dotenv\n",
        "# å¯åŠ¨ Gradio æœ‰å…¬ç½‘åœ°å€\n",
        "!python webui_mix.py --share\n"
      ],
      "metadata": {
        "id": "hNDl-5muR77-",
        "outputId": "c224df28-a728-40db-8724-9960f3146e51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChatTTS_colab\n",
            "/content/ChatTTS_colab/ChatTTS\n",
            "/content/ChatTTS_colab\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "2025-02-15 09:01:11.978289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739610072.263890    1706 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739610072.337473    1706 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-15 09:01:12.902250: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading ChatTTS model...\n",
            "INFO:ChatTTS.core:Download from HF: https://huggingface.co/2Noise/ChatTTS\n",
            "Fetching 12 files:   0% 0/12 [00:00<?, ?it/s]\n",
            "spk_stat.pt: 100% 4.26k/4.26k [00:00<00:00, 27.4MB/s]\n",
            "\n",
            "Vocos.pt:   0% 0.00/54.4M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Decoder.pt:   0% 0.00/104M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.pt:   0% 0.00/337k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.pt: 100% 337k/337k [00:00<00:00, 10.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "config%2Fdecoder.yaml: 100% 117/117 [00:00<00:00, 826kB/s]\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:   0% 0.00/60.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config%2Fdvae.yaml: 100% 143/143 [00:00<00:00, 1.09MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE.pt:   0% 0.00/27.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  10% 10.5M/104M [00:00<00:02, 41.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config%2Fgpt.yaml: 100% 346/346 [00:00<00:00, 2.30MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config%2Fpath.yaml: 100% 309/309 [00:00<00:00, 2.23MB/s]\n",
            "\n",
            "Vocos.pt:  19% 10.5M/54.4M [00:00<00:01, 36.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   1% 10.5M/901M [00:00<00:24, 36.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "DVAE_full.pt:  17% 10.5M/60.4M [00:00<00:01, 35.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config%2Fvocos.yaml: 100% 460/460 [00:00<00:00, 3.06MB/s]\n",
            "\n",
            "\n",
            "Decoder.pt:  20% 21.0M/104M [00:00<00:02, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE.pt:  38% 10.5M/27.7M [00:00<00:00, 33.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Vocos.pt:  39% 21.0M/54.4M [00:00<00:00, 41.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   2% 21.0M/901M [00:00<00:21, 41.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "DVAE_full.pt:  35% 21.0M/60.4M [00:00<00:01, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  30% 31.5M/104M [00:00<00:01, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE.pt:  76% 21.0M/27.7M [00:00<00:00, 38.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Vocos.pt:  58% 31.5M/54.4M [00:00<00:00, 40.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   3% 31.5M/901M [00:00<00:21, 40.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "DVAE_full.pt:  52% 31.5M/60.4M [00:00<00:00, 40.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DVAE.pt: 100% 27.7M/27.7M [00:00<00:00, 36.7MB/s]\n",
            "Fetching 12 files:   8% 1/12 [00:01<00:12,  1.11s/it]\n",
            "\n",
            "Decoder.pt:  40% 41.9M/104M [00:00<00:01, 42.7MB/s]\u001b[A\u001b[A\n",
            "Vocos.pt:  77% 41.9M/54.4M [00:00<00:00, 43.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   5% 41.9M/901M [00:01<00:19, 43.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "DVAE_full.pt:  69% 41.9M/60.4M [00:01<00:00, 41.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  51% 52.4M/104M [00:01<00:01, 42.7MB/s]\u001b[A\u001b[A\n",
            "Vocos.pt:  96% 52.4M/54.4M [00:01<00:00, 42.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Vocos.pt: 100% 54.4M/54.4M [00:01<00:00, 41.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "DVAE_full.pt:  87% 52.4M/60.4M [00:01<00:00, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  61% 62.9M/104M [00:01<00:00, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   7% 62.9M/901M [00:01<00:19, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "DVAE_full.pt: 100% 60.4M/60.4M [00:01<00:00, 40.7MB/s]\n",
            "Fetching 12 files:  17% 2/12 [00:01<00:08,  1.24it/s]\n",
            "\n",
            "Decoder.pt:  71% 73.4M/104M [00:01<00:00, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   8% 73.4M/901M [00:01<00:19, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  81% 83.9M/104M [00:01<00:00, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:   9% 83.9M/901M [00:01<00:19, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt:  91% 94.4M/104M [00:02<00:00, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  10% 94.4M/901M [00:02<00:18, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Decoder.pt: 100% 104M/104M [00:02<00:00, 42.3MB/s]\n",
            "Fetching 12 files:  25% 3/12 [00:02<00:07,  1.18it/s]\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  12% 105M/901M [00:02<00:18, 42.5MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  13% 115M/901M [00:02<00:18, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  14% 126M/901M [00:02<00:18, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  15% 136M/901M [00:03<00:17, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  16% 147M/901M [00:03<00:17, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  17% 157M/901M [00:03<00:17, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  19% 168M/901M [00:03<00:17, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  20% 178M/901M [00:04<00:16, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  21% 189M/901M [00:04<00:16, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  22% 199M/901M [00:04<00:16, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  23% 210M/901M [00:04<00:16, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  24% 220M/901M [00:05<00:15, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  26% 231M/901M [00:05<00:15, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  27% 241M/901M [00:05<00:15, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  28% 252M/901M [00:05<00:15, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  29% 262M/901M [00:06<00:14, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  30% 273M/901M [00:06<00:15, 41.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  31% 283M/901M [00:06<00:14, 42.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  33% 294M/901M [00:06<00:14, 42.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  34% 304M/901M [00:07<00:14, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  35% 315M/901M [00:07<00:13, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  36% 325M/901M [00:07<00:13, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  37% 336M/901M [00:07<00:13, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  38% 346M/901M [00:08<00:13, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  40% 357M/901M [00:08<00:12, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  41% 367M/901M [00:08<00:12, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  42% 377M/901M [00:08<00:12, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  43% 388M/901M [00:09<00:12, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  44% 398M/901M [00:09<00:11, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  45% 409M/901M [00:09<00:11, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  47% 419M/901M [00:09<00:11, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  48% 430M/901M [00:10<00:11, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  49% 440M/901M [00:10<00:10, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  50% 451M/901M [00:10<00:10, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  51% 461M/901M [00:10<00:10, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  52% 472M/901M [00:11<00:10, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  54% 482M/901M [00:11<00:09, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  55% 493M/901M [00:11<00:09, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  56% 503M/901M [00:11<00:09, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  57% 514M/901M [00:12<00:09, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  58% 524M/901M [00:12<00:08, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  59% 535M/901M [00:12<00:08, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  61% 545M/901M [00:12<00:08, 42.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  62% 556M/901M [00:13<00:08, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  63% 566M/901M [00:13<00:07, 42.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  64% 577M/901M [00:13<00:07, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  65% 587M/901M [00:13<00:07, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  66% 598M/901M [00:14<00:07, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  68% 608M/901M [00:14<00:06, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  69% 619M/901M [00:14<00:06, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  70% 629M/901M [00:14<00:06, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  71% 640M/901M [00:15<00:06, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  72% 650M/901M [00:15<00:05, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  73% 661M/901M [00:15<00:05, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  75% 671M/901M [00:15<00:05, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  76% 682M/901M [00:16<00:05, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  77% 692M/901M [00:16<00:04, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  78% 703M/901M [00:16<00:04, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  79% 713M/901M [00:16<00:04, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  80% 724M/901M [00:17<00:04, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  81% 734M/901M [00:17<00:03, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  83% 744M/901M [00:17<00:03, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  84% 755M/901M [00:17<00:03, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  85% 765M/901M [00:17<00:03, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  86% 776M/901M [00:18<00:02, 43.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  87% 786M/901M [00:18<00:02, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  88% 797M/901M [00:18<00:02, 43.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  90% 807M/901M [00:18<00:02, 43.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  91% 818M/901M [00:19<00:01, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  92% 828M/901M [00:19<00:01, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  93% 839M/901M [00:19<00:01, 42.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  94% 849M/901M [00:19<00:01, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  95% 860M/901M [00:20<00:01, 34.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  98% 881M/901M [00:20<00:00, 45.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt:  99% 891M/901M [00:20<00:00, 44.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "GPT.pt: 100% 901M/901M [00:21<00:00, 42.5MB/s]\n",
            "Fetching 12 files: 100% 12/12 [00:21<00:00,  1.78s/it]\n",
            "INFO:ChatTTS.core:use cuda:0\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vocos.load_state_dict(torch.load(vocos_ckpt_path))\n",
            "INFO:ChatTTS.core:vocos loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dvae.load_state_dict(torch.load(dvae_ckpt_path))\n",
            "INFO:ChatTTS.core:dvae loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  gpt.load_state_dict(torch.load(gpt_ckpt_path))\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.pretrain_models['spk_stat'] = torch.load(spk_stat_path).to(device)\n",
            "INFO:ChatTTS.core:gpt loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(decoder_ckpt_path, map_location='cpu'))\n",
            "INFO:ChatTTS.core:decoder loaded.\n",
            "/content/ChatTTS_colab/ChatTTS/core.py:144: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  tokenizer = torch.load(tokenizer_path, map_location='cpu')\n",
            "INFO:ChatTTS.core:tokenizer loaded.\n",
            "INFO:ChatTTS.core:All initialized.\n",
            "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: GET https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64 \"HTTP/1.1 200 OK\"\n",
            "* Running on public URL: https://4f53635a401a2f8a58.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.646 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.646 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "['å››å·']\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=113:   0% 0/1 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "2025-02-15 09:02:44,255 WETEXT INFO found existing fst: /usr/local/lib/python3.11/dist-packages/tn/zh_tn_tagger.fst\n",
            "INFO:wetext-zh_normalizer:found existing fst: /usr/local/lib/python3.11/dist-packages/tn/zh_tn_tagger.fst\n",
            "2025-02-15 09:02:44,256 WETEXT INFO                     /usr/local/lib/python3.11/dist-packages/tn/zh_tn_verbalizer.fst\n",
            "INFO:wetext-zh_normalizer:                    /usr/local/lib/python3.11/dist-packages/tn/zh_tn_verbalizer.fst\n",
            "2025-02-15 09:02:44,256 WETEXT INFO skip building fst for zh_normalizer ...\n",
            "INFO:wetext-zh_normalizer:skip building fst for zh_normalizer ...\n",
            "INFO:ChatTTS.core:homophones_replacer loaded.\n",
            "Inferring audio for seed=113:   0% 0/1 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2098, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1645, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 883, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 212, in audio_interface\n",
            "    seeds = generate_seeds(num_seeds, texts, progress.tqdm)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 139, in generate_seeds\n",
            "    filename = generate_audio_for_seed(chat, seed, texts, 1, 5, \"[oral_2][laugh_0][break_4]\", None, 0.3, 0.7, 20)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n",
            "['å››å·']\n",
            "speaker_type: seed\n",
            "Inferring audio for seed=4828:   0% 0/1 [00:00<?, ?steps/s]INFO:ChatTTS.core:All initialized.\n",
            "Inferring audio for seed=4828:   0% 0/1 [00:00<?, ?steps/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2098, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1645, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 883, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 212, in audio_interface\n",
            "    seeds = generate_seeds(num_seeds, texts, progress.tqdm)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/webui_mix.py\", line 139, in generate_seeds\n",
            "    filename = generate_audio_for_seed(chat, seed, texts, 1, 5, \"[oral_2][laugh_0][break_4]\", None, 0.3, 0.7, 20)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/tts_model.py\", line 110, in generate_audio_for_seed\n",
            "    wavs = chat.infer(batch, params_infer_code=_params_infer_code, params_refine_text=params_refine_text,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 259, in infer\n",
            "    return next(res_gen)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/core.py\", line 187, in _infer\n",
            "    text_tokens = refine_text(\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/content/ChatTTS_colab/ChatTTS/infer/api.py\", line 97, in refine_text\n",
            "    text_token = models['tokenizer'](text, return_tensors='pt', add_special_tokens=False, padding=True).to(device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2868, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2956, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2769, in _get_padding_truncation_strategies\n",
            "    if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n",
            "                                                           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n",
            "    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n",
            "AttributeError: BertTokenizerFast has no attribute pad_token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCJ43iVFuJl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}